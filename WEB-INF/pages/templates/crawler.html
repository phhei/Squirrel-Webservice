<div class="container">
  <div class="markdown-body">
    <h1>
      <a id="user-content-crawler" class="anchor" href="#crawler" aria-hidden="true"></a>Crawler Subgroup
    </h1>

      <p> Mentor: <a href="michael.roeder@uni-paderborn.de">Michael Röder</a>
      </p>

    <h3>
      <a id="user-content-vision" class="anchor" href="#Vision" aria-hidden="true"></a>Vision
    </h3>

      <ul>
        <li>Manageable 24/7 crawling</li>
        <li>Data is written into a large triple store</li>
      </ul>

    <h3>
      <a id="user-content-requirements" class="anchor" href="#requirements" aria-hidden="true"></a>Requirements
    </h3>

      <ul>
        <li>24/7 crawling</li>
      </ul>

      <ul>
        <li>Parts of the development of the crawler <a href="https://github.com/dice-group/Squirrel/">Squirrel(Github)</a> are carried out by Geraldo Souza, Ivan Ermilov and Michael Röder.</li>
        <li>The Frontier has to be able to handle the sudden death of a worker. Blocked IP addresses have to be given to another worker if a timeout is reached. This includes a “keep alive” mechanism with which the workers can convince the frontier that they are still working.</li>
        <li>he Frontier has to be able to recrawl old URIs to update the data. There should be a default time duration after which the recrawling takes place. But it also has to be possible to set individual recrawling times for single URIs (e.g., if the worker found this information in an HTTP header field).</li>
      </ul>
      <ul>
        <li>Frontend for Frontier</li>
      </ul>

      <ul>
        <li>Shows statistics about status</li>
      </ul>

      <ul>
        <li>Number of crawled URIs</li>
        <li>Number of blocked IPs</li>
        <li>Length of queue</li>
        <li>......</li>
        <li>Statistics about workers</li>
      </ul>

      <ul>
        <li>Provenance</li>
      </ul>

      <ul>
        <li>Handled by sink</li>
        <li>Metadata</li>
      </ul>

      <ul>
        <li>Visualize the crawled graph</li>
      </ul>

      <ul>
        <li>E.g., gexf <a href="https://gephi.org/gexf/format/">GEXF File Format</a></li>
      </ul>

      <ul>
        <li>Triple store as sink</li>
      </ul>

      <ul>
        <li>1 graph per crawled URI</li>
        <li>1 metadata graph</li>
      </ul>

      <ul>
        <li>Deduplication</li>
      </ul>

      <ul>
        <li>Can be handled by reusing the triple store sink</li>
        <li>Check randomly chosen triples against store before processing data</li>
      </ul>

      <ul>
        <li>Team abilities</li>
      </ul>

      <ul>
        <li>Necessary</li>
      </ul>

      <ul>
        <li>Java programming</li>
      </ul>
    
      <ul>
        <li>Helpful</li>
      </ul>

      <ul>
        <li>Maven</li>
        <li>Docker</li>
        <li>Java web services</li>
        <li>1 developer with JavaScript experience</li>
        <li>Team management</li>
      </ul>

      <li>Steps</li>
        <p> Get into the field of developing a crawler. An example for a linked data crawler is 
          <a href="https://github.com/ldspider/ldspider">LDSpider(Github)</a> , 
          <a href="http://iswc2010.semanticweb.org/pdf/495.pdf">LDSpider: An open-source crawling framework for the Web of Linked Data</a>. 
          However, it has its limitations.<br>      
          The deduplication is mainly build on the assumption that the sink is a triple store. All other parts have no specific order.</p>      
  </div>
</div>